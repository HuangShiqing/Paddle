/* Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. */
#ifdef PADDLE_WITH_XPU_KP

#include <xpu/runtime.h>  // NOLINT
#include <algorithm>
#include <ctime>
#include <memory>
#include <numeric>

#include "paddle/fluid/memory/memcpy.h"
#include "paddle/fluid/platform/device/xpu/enforce_xpu.h"
#include "paddle/fluid/platform/device_context.h"

#include "xpu/kernel/xtdk.h"  // NOLINT
#include "xpu/kernel/xtdk_math.h"            // NOLINT
#include "xpu/kernel/xtdk_simd.h"

#include "xpu/kernel/xtdk_io.h"

#include "paddle/fluid/operators/batch_fc_op.h"
#include "paddle/phi/kernels/funcs/math_function.h"

#include "paddle/fluid/operators/xpu_api_wrapper.h"

namespace paddle {
namespace operators {
using framework::Tensor;


static __device__ inline void memset_lm_float(float* dst_ptr, int size) {
    for (int i = 0; i < size; i += 16) {
        vstore_lm_float32x16_mz(dst_ptr + i, 0, 0);
    }
    mfence_lm();
}

template <typename T>
__global__ void add_bias_kernel(
    T* data, int slot_pairs_num, int ins_num, int out_dim, const T* bias) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
    return;
  }
  int thread_id = cluster_id() * ncores + cid;
  int nthreads = cluster_num() * ncores;

  int row_per_loop = 10;
  int total_row = slot_pairs_num * ins_num;
  // int threads_per_bs = roundup_div(ins_num, row_per_loop);
  int buf_size = row_per_loop * (out_dim + 16);
  __simd__ T local_data_buf[buf_size];
  __simd__ T local_bias_buf[out_dim + 16];

  __simd__ T out_buf[buf_size];
  memset_lm_float(out_buf, buf_size);

  for (int g_index = thread_id * row_per_loop; g_index < total_row; g_index += nthreads * row_per_loop) {
    int b_index = g_index / ins_num; // bs index
    int r_index = g_index % ins_num; // row index

    GM2LM_ASYNC(bias + b_index, local_bias_buf,  out_dim * sizeof(T));

    int gm_offset = b_index * ins_num * out_dim + r_index * out_dim;
    GM2LM_ASYNC(data + gm_offset, local_data_buf, row_per_loop * out_dim * sizeof(T));

    mfence();

    int col_offset = 0;
    for (int curr_row = 0; curr_row < row_per_loop; curr_row++) {
      // column
      for (int col_step = 0; col_step < out_dim; col_step += 16) {
        col_offset = curr_row * out_dim + col_step;
        float32x16_t vec_data = vload_lm_float32x16(local_data_buf + col_offset);
        float32x16_t vec_bias = vload_lm_float32x16(local_bias_buf);

        vec_data = vvadd_float32x16(vec_data, vec_bias);
        vstore_lm_float32x16(out_buf + col_step, vec_data);
      }
      mfence();
      LM2GM_ASYNC(out_buf, data + gm_offset, out_dim * sizeof(T));
      mfence();
    }
  }
}

template <typename T>
void add_bias(xpu::Context* xpu_ctx,
              T* data,
              int slot_pairs_num,
              int ins_num,
              int out_dim,
              const T* bias) {
  auto stream = xpu_ctx->xpu_stream;              
  add_bias_kernel<<<8, 64, stream>>>(data, slot_pairs_num, ins_num, out_dim, bias);
}

template <typename T>
__global__ void add_bias_grad_kernel(const T* dout_data,
                                     int slot_pairs_num,
                                     int ins_num,
                                     int out_dim,
                                     T* db_data) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
    return;
  }
  int thread_id = cluster_id() * ncores + cid;
  int nthreads = cluster_num() * ncores;

  int bs_per_loop = 64;
  int total_bs = slot_pairs_num * out_dim;
  int buf_size = bs_per_loop;
  __simd__ T local_bias_buf[buf_size];
  __simd__ T tmp_sum_buf[buf_size];

  __local__ float local_data_buf[1];

  memset_lm_float(local_bias_buf, buf_size);
  memset_lm_float(tmp_sum_buf, buf_size);

  __local__ T tmp_sum = static_cast<T>(0);
  for (int g_index = thread_id * bs_per_loop; g_index < total_bs; g_index += nthreads * bs_per_loop) {
    int len = min(total_bs - g_index, bs_per_loop);
    int r_index = g_index / out_dim; // row index
    int c_index = g_index % out_dim; // col index

    GM2LM_ASYNC(db_data + g_index, local_bias_buf, len * sizeof(T));

    for (int index = 0; index < len; index++) {
      for (int i = 0; i < ins_num; ++i) { 
        int select_indx = ((r_index + 1) * i + 1) * c_index;
        GM2LM_ASYNC(dout_data + select_indx, local_data_buf, sizeof(T));
        mfence();
        tmp_sum_buf[index] += local_data_buf[0];
      }
    }

    mfence();

    for (int step = 0; step < len; step += 16) {
        float32x16_t vec_bias = vload_lm_float32x16(local_bias_buf + step);
        float32x16_t vec_sum = vload_lm_float32x16(tmp_sum_buf + step);
        vec_bias = vvadd_float32x16(vec_bias, vec_sum);
        vstore_lm_float32x16(local_bias_buf + step, vec_bias);
    }
    mfence();
    LM2GM_ASYNC(local_bias_buf, db_data + g_index, len * sizeof(T));
  }
}

template <typename T>
void add_bias_grad(xpu::Context* xpu_ctx,
                   const T* dout_data,
                   int slot_pairs_num,
                   int ins_num,
                   int out_dim,
                   T* db_data) {
  auto stream = xpu_ctx->xpu_stream;   
  add_bias_grad_kernel<<<8, 64, stream>>>(
      dout_data, slot_pairs_num, ins_num, out_dim, db_data);
}



template <typename DeviceContext, typename T>
class BatchFCXPUKernel : public framework::OpKernel<T> {
  using XPUType = typename XPUTypeTrait<T>::Type;

 public:
  void Compute(const framework::ExecutionContext& ctx) const override {
    int batchcount = ctx.Attr<int>("batchcount");
    auto transpose_weight = ctx.Attr<bool>("transpose_weight");
    if (transpose_weight) {
      // TODO
      PADDLE_ENFORCE_EQ(
        transpose_weight,
        true,
        platform::errors::Unimplemented("BatchFC not support transpose_weight now."));
      return;
    }
    if (batchcount > 0) {
      // TODO
      PADDLE_ENFORCE_EQ(
        (batchcount > 0),
        true,
        platform::errors::Unimplemented("BatchFC not support transpose_weight now."));
    } else {
      // X.dim = slot_pairs_num * ins_num * in_dim
      // W.dim = slot_pairs_num * in_dim * out_dim
      // b.dim = slot_pairs_num * out_dim
      // output.dim = slot_pairs_num * ins_num * out_dim
      auto* input = ctx.Input<framework::LoDTensor>("Input");
      auto* w = ctx.Input<Tensor>("W");
      auto* bias = ctx.Input<Tensor>("Bias");
      auto* output = ctx.Output<framework::LoDTensor>("Out");
      auto input_dims = input->dims();
      auto w_dims = w->dims();
      auto slot_pairs_num = input_dims[0];
      auto ins_num = input_dims[1];
      // auto in_dim = input_dims[2];
      auto out_dim = w_dims[2];
  
      // get data ptr
      const XPUType* x_ptr = reinterpret_cast<const XPUType*>(input->data<T>());
      const XPUType* y_ptr = reinterpret_cast<const XPUType*>(w->data<T>());
      const XPUType* bias_data = reinterpret_cast<const XPUType*>(bias->data<T>());
  
      output->Resize({slot_pairs_num, ins_num, out_dim});
      XPUType* out_ptr = reinterpret_cast<XPUType*>(output->mutable_data<T>(ctx.GetPlace()));

      // initialize
      auto& dev_ctx =
        ctx.template device_context<DeviceContext>();
      xpu::Context* xpu_ctx = dev_ctx.x_context();
  
      bool trans_x = false;
      bool trans_y = false;
  
      T alpha = 1;
      T beta = 0;
     
      XpuFcInfo fc_info;
      GetFCInfo(input_dims, w_dims, trans_x, trans_y, &fc_info);
      MatMulXPUFunction<XPUType>(xpu_ctx, x_ptr, y_ptr, out_ptr, fc_info, alpha);
  
      // add bias
      add_bias<T>(xpu_ctx,
                  out_ptr,
                  slot_pairs_num,
                  ins_num,
                  out_dim,
                  bias_data);
    }
  }
};

template <typename DeviceContext, typename T>
class BatchFCGradOpXPUKernel : public framework::OpKernel<T> {
  using XPUType = typename XPUTypeTrait<T>::Type;

 public:
  void Compute(const framework::ExecutionContext& ctx) const override {
    int batchcount = ctx.Attr<int>("batchcount");
    if (batchcount > 0) {
      // TODO
      PADDLE_ENFORCE_EQ(
        (batchcount > 0),
        true,
        platform::errors::Unimplemented("BatchFC not support transpose_weight now."));
    } else {
      auto* input = ctx.Input<Tensor>("Input");
      auto* w = ctx.Input<Tensor>("W");
      auto* dout = ctx.Input<Tensor>(framework::GradVarName("Out"));
  
      auto* dx = ctx.Output<Tensor>(framework::GradVarName("Input"));
      auto* dw = ctx.Output<Tensor>(framework::GradVarName("W"));
      auto* db = ctx.Output<Tensor>(framework::GradVarName("Bias"));
  
      auto input_dims = input->dims();
      auto w_dims = w->dims();
      auto slot_pairs_num = input_dims[0];
      auto ins_num = input_dims[1];
      // auto in_dim = input_dims[2];
      auto out_dim = w_dims[2];

      const XPUType* dout_ptr = reinterpret_cast<const XPUType*>(dout->data<T>());
      const XPUType* x_ptr = reinterpret_cast<const XPUType*>(dx->mutable_data<T>(ctx.GetPlace()));
      const XPUType* y_ptr = reinterpret_cast<const XPUType*>(dw->mutable_data<T>(ctx.GetPlace()));
      XPUType* b_ptr = reinterpret_cast<XPUType*>(db->mutable_data<T>(ctx.GetPlace()));

      auto& dev_ctx =
        ctx.template device_context<DeviceContext>();
      xpu::Context* xpu_ctx = dev_ctx.x_context();
      xpu::ctx_guard RAII_GUARD(xpu_ctx);

      bool transpose_x = false;
      bool transpose_y = false;
      XpuFcInfo info_forward;
      GetFCInfo(input_dims, w_dims, transpose_x, transpose_y, &info_forward);

      const XPUType* a_1 = reinterpret_cast<const XPUType*>(NULL);
      const XPUType* b_1 = reinterpret_cast<const XPUType*>(NULL);
      const XPUType* a_2 = reinterpret_cast<const XPUType*>(NULL);
      const XPUType* b_2 = reinterpret_cast<const XPUType*>(NULL);
      XPUType* c_1 = (dx == NULL) ? reinterpret_cast<XPUType*>(NULL)
                                  : reinterpret_cast<XPUType*>(dx->data<T>());
      XPUType* c_2 = (dw == NULL) ? reinterpret_cast<XPUType*>(NULL)
                                  : reinterpret_cast<XPUType*>(dw->data<T>());
                                  
      // add bias grad
      db->mutable_data<T>(ctx.GetPlace());
      add_bias_grad<T>(xpu_ctx,
                       dout_ptr,
                       slot_pairs_num,
                       ins_num,
                       out_dim,
                       b_ptr);
  
      T alpha = 1;

      XpuFcInfo info_dx;
      XpuFcInfo info_dy;
      std::tuple<XpuFcInfo,
                XpuFcInfo,
                const XPUType*,
                const XPUType*,
                const XPUType*,
                const XPUType*>
          fc_info = MatmulGradFcInfo(xpu_ctx,
                                    &RAII_GUARD,
                                    info_forward,
                                    transpose_x,
                                    transpose_y,
                                    x_ptr,
                                    y_ptr,
                                    dout_ptr);
      std::tie(info_dx, info_dy, a_1, b_1, a_2, b_2) = fc_info;
  
      // dx = dout_data * y^T
      MatMulXPUFunction<XPUType>(xpu_ctx, a_1, b_1, c_1, info_dx, alpha);
      
      // dy = x^T * dout_data
      MatMulXPUFunction<XPUType>(xpu_ctx, a_2, b_2, c_2, info_dy, alpha);
    }
  }
};

}  // namespace operators
}  // namespace paddle

     

namespace ops = paddle::operators;
namespace plat = paddle::platform;

REGISTER_OP_KERNEL(batch_fc, KP, plat::XPUPlace,
                   ops::BatchFCXPUKernel<paddle::platform::XPUDeviceContext, float>);     
REGISTER_OP_KERNEL(batch_fc_grad, KP, plat::XPUPlace,
                   ops::BatchFCGradOpXPUKernel<paddle::platform::XPUDeviceContext, float>);    

// namespace ops = paddle::operators;

REGISTER_OP_XPU_KERNEL(batch_fc,
                       ops::BatchFCXPUKernel<paddle::platform::XPUDeviceContext, float>);      
REGISTER_OP_XPU_KERNEL(batch_fc_grad,
                       ops::BatchFCGradOpXPUKernel<paddle::platform::XPUDeviceContext, float>);  
#endif